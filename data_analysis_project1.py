"""Data_analysis_project1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jjDAieD6nsTfkdE3FLST4ES3994y-bA0

#       Project of CLUSTERING / KMEANS (ADD) :
#OUIAME BAHOU LSD2

# Part 1 :

We will start off by importing all the libraries needed in this specific part :
"""

import pandas as pd
import numpy as np
from numpy import linalg as LA
from numpy.random import default_rng
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans

rng = default_rng(12345)

"""After importing all the libraries needed,we will now create a function called three_clusters that takes as an input the dimension d and number n of vectors used as well as w which is an integer that will be multiplied by e=[1,1,0,....,0]that has d as a dimension.In fact,-1*w*e will be the center of cluster X1 ;2*w*e will be the center of the second cluster and 3*e*w will be the center of the cluster X3.

The vectors of each of our clusters follow the same law (THE NORMAL LAW N(0,1)).

Now in order to generate these three clusters , we will be using "np.concatenate" and "np.zeros(d-1)" which will be creating the centers of each of our clusters,and of course we add it to rng.normal(0,1,d) which assures that the vectors of each cluster follow the normal law .
We can't forget to apply "np.stack()" to the whole in order to create the matrixes needed.
"""

def three_clusters(d, n, w):
    X1 = np.stack([np.concatenate(([-w], np.zeros(d-1))) + rng.normal(0,1,d) for _ in range(n)])
    X2 = np.stack([np.concatenate(([2*w], np.zeros(d-1))) + rng.normal(0,1,d) for _ in range(n)])
    X3 = np.stack([np.concatenate(([3*w], np.zeros(d-1))) + rng.normal(0,1,d) for _ in range(n)])
    return X1, X2 ,X3

"""Now that we have created our function three_clusters successfully, we will test it with an example.

In this example,we have : d=2,n=100,w=1. So we affect to X1, X2 ,X3 the output of our function three_clusters and we concatenate the three matrixes and then visualise it all by "plt.scatter()".
"""

d, n, w = 2, 100, 1.
X1, X2 ,X3 = three_clusters(d, n, w)
X = np.concatenate((X1, X2 ,X3), axis=0)
plt.scatter(X[:,0], X[:,1])
plt.show()

"""Now ,we will be generating the algorithm of kmeans.

To give a little bit of background, clustering is by defnition the classification of data or grouping unlabeled examples. So basically u have points representing data in a graph and you want to classify them and seperate them(We use colors usually to differentiate between each cluster).Let's say for an example you own a restaurant and you have a certain number of clients that visits your restaurant frequently and you want to classify them based on what each of them prioritizes while being your client (Let's say a 1/3 of clients cares mostly about low prices,the second 1/3 cares mostly about quick and in time delievery while the last 1/3 cares more so about great quality) in order to serve each group the way they wanna be treated and consequently manage your restaurant in a better way .Here CLUSTERING comes in handy.

KMeans is one of the three algorithms that help establish CLUSTERING.

Usually, the kmeans algorithm consists of five  steps (2 crucial repetitive operations) :


1.We start off by choosing the number of clusters that we want ,thus the number of centroids that we need( let's take the example of three clusters neeeded, thus three centroids).


2.Now,three clusters get chosen randomly (they can be far away from the data).


3.the distance between each point and the three centroids gets calculated and the smallest one is the one we need. Now the centroid corresponding to the smallest distance is the one that that point belongs to and it gets colored with its same color to differentiate between clusters and make them visible when visualising.Same thing happens to each point and by the end , the data is all colored.


4.Now, the centroids get translated more closer to the data or the points representing the cluster(Where there is the most of them).Same happens to the three centers.


5.Now,since the centroids moved, if we recalculate the distances in between the dots and the three centers , we would remark that for some of them, the smallest distance isn't the one between them and the center they belonged to before ; and for that we redo here step 3 all over again .After that, we redo step 4 (Translation of centroids) and we contiue on redoing these major two operations until the centroids are in the center of their data all while the distance between them and each point belonging to their cluster is the smallest .

In order to create the algorithm of kmeans, we will first create a function that we will be called :opt_clust.

This function takes as an input X the sample ,K the number of clusters to form as well as reps which is the matrix (k,d) containing in its ith row the ith centroid's coordinates.
We shape our X into a matrix (n,d) so we can be able to manipulate its row vectors :For each row vector , we define dist_to_j which will contain in each row ,the distance between Xi and the centeroid .We define assign[i] as the index of the cluster that each point belongs to .
We return assign which contains in row i  the index of the cluster to which belongs the vector i in each iteration.

This function does the first major operation in the kmeans algorithm (choosing a certain number of centroids,calculating the distances ,choosing the smallest one and referring it to that centroid which distance was the smallest ).
"""

def opt_clust(X, k, reps):
    (n, d) = X.shape
    dist = np.zeros(n)
    assign = np.zeros(n, dtype=int)
    for i in range(n):
        dist_to_j = [LA.norm(X[i,:] - reps[j,:]) for j in range(k)]
        assign[i] = np.argmin(dist_to_j)
        dist[i] = dist_to_j[assign[i]]
    G = np.sum(dist ** 2)
    print(G)
    return assign

"""Now we will be creating a function which is called opt_reps.
This function takes as an input our infamous sample X, k representing the number of clusters and assign representing the matrix returned from the function opt_clust.
We shape our sample X into a matrix (n,d), then we initiate the matrix reps (k,d).
After that , we proceed to affect the indexes of all vectors i of the cluster of index j to "in_j" as well as affecting the centroid of the specific cluster to "reps[j,:]"(representing the jth row of reps).
Lastly, we return the centroid of this specific cluster .

THis function takes care of accomplishing the second major step of the kmeans algorithm (the translation of cneters initially picked until we find their perfect position).
"""

def opt_reps(X, k, assign):
    (n, d) = X.shape
    reps = np.zeros((k, d))
    for j in range(k):
        in_j = [i for i in range(n) if assign[i] == j]
        reps[j,:] = np.sum(X[in_j,:],axis=0) / len(in_j)
    return reps

"""Now we create the function mmids_kmeans that will combain the two functions previously created as well as complete the algorithm.

"""

def mmids_kmeans(X, k, maxiter=10):
  colormap=np.array(['tomato','royalblue','chartreuse'])
  fig,axs=plt.subplots(1,10,sharex=True,figsize=(30,9))
  (n, d) = X.shape
  assign = rng.integers(0,k,n)
  reps = np.zeros((k, d), dtype=int)
  for iter in range(maxiter):
        # Step 1: Optimal representatives for fixed clusters
      reps = opt_reps(X, k, assign)
        # Step 2: Optimal clusters for fixed representatives
      assign = opt_clust(X, k, reps)
      axs[iter].scatter(X[:,0], X[:,1], c=colormap[assign])
      axs[iter].scatter(reps[:,0], reps[:,1],marker='x',color='black',s=60)
  return assign

"""Here we applicate the previous function for k=3 and d=100 :"""

assign = mmids_kmeans(X, 3)

"""Here we applicate the kmeans algo for d=1000 :"""

D, N, W =1000, 100, 1.
X1, X2 ,X3 = three_clusters(D, N, W)
X = np.concatenate((X1, X2 ,X3), axis=0)
plt.scatter(X[:,0], X[:,1])
plt.show()

assign = mmids_kmeans(X, 3)

"""As we can clearly observe, the classification is no longer efficient for a high dimension (example of d=1000).So , in order to solve this issue we need to use some dimension-reduction techniques.

# Observation and elaboration : #
For the two visualizations observed above ,we can see that in the first iteration ,  the initial centroids get formed;then for iteration number two : we see the assignment of the points representing the data to the nearest centroid to each one of them;after that we can observe the rellocation of the centroids(translation);and in the third iteration , the same two steps get repeated(assignement->rellocation),same for the third one ...
#conclusion :#
So the conclusion is that at first the centroids get formed and then for the rest of the nine iterations ,the same two steps mentioned above keep on happening each time until the three clusters are clear and well seperated and the centeroids are in their perfet positions(the center of the points representing the cluster).

Now we will be using KMeans from the sklearn library.

We group the points with k-means and  k=3 and d=10000 ;
The result is three centroids around which the points are grouped and each point its label that indicate which cluster this point belongs to.
We use scatter to visualize the three clusters as well as their centeroids .
"""

from sklearn.cluster import KMeans
colormap=np.array(['tomato','royalblue','chartreuse'])
kmeans = KMeans(n_clusters=3, random_state=0).fit(X)
a=kmeans.labels_
b=kmeans.cluster_centers_
plt.scatter(X[:,0], X[:,1], c=colormap[a])
plt.scatter(b[:,0], b[:,1], marker= 'x' ,color='black')

"""# Part 2 :#

Just like what we did in part 1 ; we will start off here by importing the librairies necessary for this part (The usual libraries + the library that contains KMeans + the libray containing the data set ) :

We load the digit images ; then we put them in a file data so we can manipulate them a lot easier.
Pixels from the square image of  8Ã—8  pixels have been reshaped in a row of  64  elements. Therefore, each row is an object or data. The characteristics or properties of each object are the gray intensities of each pixel. With that being said , we have, for each image,  64  properties.

To improve the visualization, we invert the colors.
We fix the seed to obtain the initial centroids, so the results obtained here are repeatable .
"""

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.cluster import KMeans
from sklearn.datasets import load_digits
digits = load_digits()
df =pd.DataFrame(digits['data'])
df=255-df
print(digits.data.shape)

plt.gray()
np.random.seed(1)
df

"""We convert the dataframe into a numpy array  so we can manipulate in the rest of the part using some numpy functions."""

R=df.to_numpy()
R

"""Since we have 1O digits (from 0 to 9); we would choose to group them in 10 clusters and classify them using KMeans ."""

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=10 , random_state=0).fit(R)
Z= kmeans.labels_
U= kmeans.cluster_centers_
Z

"""This code helps plotting the resulting clusters :


We make a loop ; we start off by storing the indexes of points (data) for the cluster i  in row; then we affect to num the number of elements in the cluster i and to r the number of rows in the cluster's figure.


We print for each cluster the number of elements belonging to it (num).


Now for the second loop inside the first loop ;we precise the number of clomuns and rows for each of our clusters as well as the index for each image of the cluster .


We define image as the image of a number belonging to the cluster i and we affectL[row[k]]
to it.


Since the images that we have are of (8,8) for each ;we give it the shape (8,8).


We visualize the images using axis off.
"""

for i in range(0,n):

    row = np.where(Z==i)[0]  # row in Z for elements of cluster i
    num = row.shape[0]       #  number of elements for each cluster
    r = np.floor(num/10.)    # number of rows in the figure of the cluster

    print("cluster "+str(i))
    print(str(num)+" elements")

    plt.figure(figsize=(10,10))
    for k in range(0, num):
        plt.subplot(r+1, 10, k+1)
        image = R[row[k], ]
        image = image.reshape(8, 8)
        plt.imshow(image, cmap='gray')
        plt.axis('off')
    plt.show()

"""To represent the image of each centroid of each cluster ; we use the inner loop of the previous code :"""

fig,axis = plt.subplots(10,1)
for k in range(10):
  image = U[k,:]
  image = image.reshape(8,8)
  axis[k].imshow(image,cmap='gray')
  plt.axis('off')

"""#Part 3 : #

And again ; just like we did for the last two parts ; we will start off by importing the libraries needed (In this part , we will import the Python Imaging Library ( PIL) ) .
"""

import PIL

"""We will start off by uploading our image named "mica" in our colab ; then we will be visualizing it :"""

I = PIL.Image.open('mica.jpg')
I
#np.shape(image)

plt.figure(figsize=(8,8))
plt.imshow(I,cmap='gray')
plt.axis('off')
plt.show()

"""Now , first things first;in order to simplify the problem , we will convert our image to shades of grey, visualize it and we will also convert the image to numpy array :"""

from numpy import array
I = I.convert('L')
Ia = array(I)

plt.figure(figsize=(8,8))
plt.imshow(Ia,cmap='gray')
plt.axis('off')
plt.show()

"""Now, we will be using reshape() and applying it to I in order to reshape the matrix to apply k-means. Now it has as many rows as pixels but only one column, the gray intensity ."""

X=Ia.reshape((-1,1))
X

"""We group the pixels into three clusters with k-means ."""

kmeans = KMeans(n_clusters=3 , random_state=0).fit(X)

"""We extract the centroids and labels for each pixel."""

labels=kmeans.labels_
centroids=kmeans.cluster_centers_

"""We create the image I1 using the code given :We build the image using only the three intensities of the centroids."""

import numpy as np
I1 = np.choose(labels, centroids)
I1.shape = Ia.shape
I1

print(I1.shape)

""" We represent now the image that we rebuilt into black and white(We can clearly observe the pixels using a figure size of (17,17))."""

plt.figure(figsize=(17,17))
plt.imshow(I1,cmap='gray')
plt.axis('off')

plt.show()

"""Now we will calculate the number of the grey shades in I1 :"""

ns=np.unique(I1) #number of shades
ns.size
print("the number of shades of grey is :",ns.size)

"""Now, in order to calculate the percentage of mica in the granite image; we will first count the number of pixels in each of the three levels of grey :"""

I1 = (I1-np.min(I1))/(np.max(I1)-np.min(I1))*255
I1 = Image.fromarray(I1.astype(np.uint8))
w, h =I1.size
colors = I1.getcolors(w * h)
print (colors)

"""There are 22500 pixels . We calculate the percentage with respect to the total number of pixels in the image and we have the percentage of the mica with respect to the total area represented by the image :"""

print ('The percentage of mica in the granite  is : ',  ((5271)/(5271+6985+10244))*100)
